{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 0. Notation\n",
        "\n",
        "m: number of examples\n",
        "\n",
        "$n_x$: number of variables(features)\n",
        "\n",
        "$n_y$: number of output(classes) \n",
        "\n",
        "x.shape: (n_x, m)\n",
        "\n",
        "y.shape: (n_y, m)\n",
        "\n",
        "W: (n_out, n_in)\n",
        "\n",
        "b: (n_out, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Forward Propagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1-1. Initialize parameters\n",
        "- Weights and biases are parameters that model connections between different layers\n",
        "- Define parameter shape: keep the implementation as close as possible to the mathmatical calculations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def initialize_weights(outs, ins):\n",
        "    return np.random.default_rng().normal(loc=0, scale=1/(outs * ins), size=(outs, ins))\n",
        "    # return np.zeros((outs, ins))\n",
        "\n",
        "def initialize_bias(outs):\n",
        "    \"\"\"create a column vector as a matrix\"\"\"\n",
        "    # return np.zeros((outs, 1))\n",
        "    return initialize_weights(outs, 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rows: 2\n",
            "cols: 4\n",
            "\n",
            "weight W:\n",
            " [[ 0.15367383  0.09207928  0.20023608  0.06530946]\n",
            " [ 0.20515677 -0.01647407  0.10064682 -0.01004938]]\n",
            "W.shape: (2, 4)\n",
            "\n",
            "bias b:\n",
            " [[ 0.49441608]\n",
            " [-0.57452146]]\n",
            "b.shape: (2, 1)\n"
          ]
        }
      ],
      "source": [
        "nrows = 2 # output number\n",
        "ncols = 4 # input number\n",
        "\n",
        "W = initialize_weights(nrows, ncols)\n",
        "b = initialize_bias(nrows)\n",
        "\n",
        "print(\"rows:\", nrows)\n",
        "print(\"cols:\", ncols)\n",
        "print()\n",
        "print(\"weight W:\\n\", W)\n",
        "print(\"W.shape:\", W.shape)\n",
        "print()\n",
        "print(\"bias b:\\n\", b)\n",
        "print(\"b.shape:\", b.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1-2. Update neuron states"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1-2-1. Linear Model; z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input x:\n",
            " [[1]\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n",
            "x.shape: (4, 1)\n"
          ]
        }
      ],
      "source": [
        "x = np.array((1, 0, 0, 0), ndmin=2).T\n",
        "\n",
        "print(\"input x:\\n\", x)\n",
        "print(\"x.shape:\", x.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "z :\n",
            " [[ 0.64808991]\n",
            " [-0.36936469]]\n",
            "z.shape: (2, 1)\n"
          ]
        }
      ],
      "source": [
        "z = np.dot(W, x) + b\n",
        "\n",
        "print(\"z :\\n\", z)\n",
        "print(\"z.shape:\", z.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1-2-2. Activation function; a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def leaky_relu(x, leaky_param=0.1):\n",
        "    return np.maximum(x, x * leaky_param)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a:\n",
            " [[ 0.64808991]\n",
            " [-0.03693647]]\n",
            "a.shape: (2, 1)\n"
          ]
        }
      ],
      "source": [
        "a = leaky_relu(z)\n",
        "\n",
        "print(\"a:\\n\", a)\n",
        "print(\"a.shape:\", a.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1-3. Modelling layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Layer:\n",
        "    \"\"\"\n",
        "    Layer class that represents the connections and the flow of information between a column of neurons and the next.\n",
        "    It deals with what happens in between two columns of neurons instead of having the layer specifially represent the neurons of each vertical column\n",
        "    \"\"\"\n",
        "    def __init__(self, ins, outs, act_function) -> None:\n",
        "        self.ins = ins\n",
        "        self.outs = outs\n",
        "        self.act_function = act_function\n",
        "\n",
        "        self._W = initialize_weights(self.outs, self.ins)\n",
        "        self._b = initialize_bias(self.outs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        helper method that computes the forward pass in the layer\n",
        "\n",
        "        Parameters:\n",
        "        x: a set of neuron states\n",
        "\n",
        "        Returns:\n",
        "        the next set of neuron states\n",
        "        \"\"\"\n",
        "        return self.act_function(np.dot(self._W, x) + self._b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1-3. Layer chain demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x:\n",
            " [[0.82533845 0.09378596 0.82585421 0.32791931 0.77569172]\n",
            " [0.69685595 0.92216848 0.52935385 0.00892833 0.64237043]]\n",
            "x.shape: (2, 5)\n",
            "\n",
            "output:\n",
            " [[-0.02081754 -0.02086665 -0.02079664 -0.02070311 -0.02081217]] (1, 5)\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    \"\"\"\n",
        "    Demo of chaining layers with compatible shapes.\n",
        "    \"\"\"\n",
        "    input_shape = 2 # n_x\n",
        "    output_shape = 1 # n_y\n",
        "\n",
        "    l1 = Layer(input_shape, 4, leaky_relu)\n",
        "    l2 = Layer(4, 4, leaky_relu)\n",
        "    l3 = Layer(4, output_shape, leaky_relu)\n",
        "\n",
        "    # x.shape (n_x, m)\n",
        "    x = np.random.uniform(size=(input_shape, 5))\n",
        "    print(\"x:\\n\", x)\n",
        "    print(\"x.shape:\", x.shape)\n",
        "\n",
        "    # y.shape (n_y, m)\n",
        "    y = l3.forward(l2.forward(l1.forward(x)))\n",
        "    print(\"\\noutput:\\n\", y, y.shape)\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Network and Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2-1. Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NeuralNet:\n",
        "    \"\"\"\n",
        "    A series of layers connected and compatible.\n",
        "    \"\"\"\n",
        "    def __init__(self, layers) -> None:\n",
        "        self._layers = layers\n",
        "        self.check_layer_compatibility()\n",
        "\n",
        "    def check_layer_compatibility(self):\n",
        "        for from_, to_ in zip(self._layers[:-1], self._layers[1:]):\n",
        "            print(\"from, to:\", from_.ins, to_.ins)\n",
        "            if from_.outs != to_.ins:\n",
        "                raise ValueError(\"Layers should have compatible shapes.\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x\n",
        "        for layer in self._layers:\n",
        "            out = layer.forward(out)\n",
        "        return out\n",
        "\n",
        "    def loss(self, y_pred, y):\n",
        "        return self._loss_function(y_pred, y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2-1. Neural Net demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "from, to: 2 4\n",
            "from, to: 4 4\n",
            "x:\n",
            " [[0.24436777 0.80835584 0.82807147]\n",
            " [0.38003572 0.1044006  0.18070241]]\n",
            "x.shape: (2, 3)\n",
            "\n",
            "y:\n",
            " [[0.01146712 0.01146167 0.01146875]] (1, 3)\n"
          ]
        }
      ],
      "source": [
        "def nn_demo():\n",
        "    \"\"\"\n",
        "    Demo of a network as a serias of layers.\n",
        "    \"\"\"\n",
        "    input_shape = 2 # n_x\n",
        "    output_shape = 1 # n_y\n",
        "\n",
        "    net = NeuralNet(\n",
        "        [\n",
        "            Layer(input_shape, 4, leaky_relu),\n",
        "            Layer(4, 4, leaky_relu),\n",
        "            Layer(4, output_shape, leaky_relu),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # x (n_x, m) \n",
        "    x = np.random.uniform(size=(input_shape, 3))\n",
        "    print(\"x:\\n\", x)\n",
        "    print(\"x.shape:\", x.shape)\n",
        "\n",
        "    # y (n_y, m)\n",
        "    y = net.forward(x) \n",
        "    print(\"\\ny:\\n\", y, y.shape)\n",
        "    \n",
        "nn_demo()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "from, to: 2 5\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Layers should have compatible shapes.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Layer compatibility test\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m net \u001b[38;5;241m=\u001b[39m \u001b[43mNeuralNet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleaky_relu\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleaky_relu\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[10], line 7\u001b[0m, in \u001b[0;36mNeuralNet.__init__\u001b[0;34m(self, layers)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, layers) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers \u001b[38;5;241m=\u001b[39m layers\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_layer_compatibility\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[10], line 13\u001b[0m, in \u001b[0;36mNeuralNet.check_layer_compatibility\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom, to:\u001b[39m\u001b[38;5;124m\"\u001b[39m, from_\u001b[38;5;241m.\u001b[39mins, to_\u001b[38;5;241m.\u001b[39mins)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_\u001b[38;5;241m.\u001b[39mouts \u001b[38;5;241m!=\u001b[39m to_\u001b[38;5;241m.\u001b[39mins:\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayers should have compatible shapes.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mValueError\u001b[0m: Layers should have compatible shapes."
          ]
        }
      ],
      "source": [
        "# Layer compatibility test\n",
        "net = NeuralNet(\n",
        "    [\n",
        "        Layer(2, 4, leaky_relu),\n",
        "        Layer(5, 1, leaky_relu),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2-2. Evaluate performance - Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def MSE(y_pred, y):\n",
        "    return np.mean((y_pred - y) ** 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NeuralNet:\n",
        "    \"\"\"\n",
        "    A series of layers connected and compatible.\n",
        "    \"\"\"\n",
        "    def __init__(self, layers, loss_function) -> None:\n",
        "        self._layers = layers\n",
        "        self._loss_function = loss_function\n",
        "        self.check_layer_compatibility()\n",
        "\n",
        "    def check_layer_compatibility(self):\n",
        "        for from_, to_ in zip(self._layers[:-1], self._layers[1:]):\n",
        "            print(\"from, to:\", from_.ins, to_.ins)\n",
        "            if from_.outs != to_.ins:\n",
        "                raise ValueError(\"Layers should have compatible shapes.\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x\n",
        "        for layer in self._layers:\n",
        "            out = layer.forward(out)\n",
        "        return out\n",
        "\n",
        "    def loss(self, y_pred, y):\n",
        "        return self._loss_function(y_pred, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2-3. Derivative"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2-3-1. Derivative of activation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def d_leaky_relu(x, leaky_param=0.1):\n",
        "    return np.maximum(x > 0, leaky_param)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.1, 0.1, 1. , 1. ])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = np.array([-2, -1, 3, 4])\n",
        "x\n",
        "d_leaky_relu(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2-3-2. Derivative of loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def MSE_derivative(y_pred, y):\n",
        "    return 2 * (y_pred - y) / y_pred.size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.5, 1. , 1.5, 2. ])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = np.array([1,2,3,4])\n",
        "x\n",
        "t = np.zeros(shape=4)\n",
        "t\n",
        "MSE_derivative(x, t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2-4. Generic Classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2-4-1. Generic Activation class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "from abc import abstractmethod\n",
        "\n",
        "class Activation:\n",
        "    \"\"\"\n",
        "    Class to be inherited by activation functions.\n",
        "    \"\"\"\n",
        "    @abstractmethod\n",
        "    def f(self, x):\n",
        "        \"\"\"\n",
        "        Method that implements the function.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def df(self, x):\n",
        "        \"\"\"\n",
        "        Derivative of the function with respect to its input.\n",
        "        \"\"\"\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LeakyReLU(Activation):\n",
        "    \"\"\"\n",
        "    Leaky Rectified Linear Unit.\n",
        "    \"\"\"\n",
        "    def __init__(self, leaky_param=0.1):\n",
        "        self.alpha = leaky_param\n",
        "\n",
        "    def f(self, x):\n",
        "        return np.maximum(x, x * self.alpha)\n",
        "\n",
        "    def df(self, x):\n",
        "        return np.maximum(x > 0, self.alpha)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2-4-2. Generic Loss class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Loss:\n",
        "    \"\"\"\n",
        "    Class to be inherited by loss functions.\n",
        "    \"\"\"\n",
        "    @abstractmethod\n",
        "    def loss(self, y_pred, y):\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def dloss(self, y_pred, y):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MSE(Loss):\n",
        "    def loss(self, y_pred, y):\n",
        "        return np.mean((y_pred - y) ** 2)\n",
        "\n",
        "    def dloss(self, y_pred, y):\n",
        "        return 2 * (y_pred - y) / y_pred.size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2-5. Update Layer and NeuralNet classes acorrding to Activation and Loss classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Layer:\n",
        "    \"\"\"\n",
        "    Layer class that represents the connections and the flow of information between a column of neurons and the next.\n",
        "    It deals with what happens in between two columns of neurons instead of having the layer specifially represent the neurons of each vertical column\n",
        "    \"\"\"\n",
        "    def __init__(self, ins, outs, act_function) -> None:\n",
        "        self.ins = ins\n",
        "        self.outs = outs\n",
        "        self.act_function = act_function\n",
        "\n",
        "        self._W = initialize_weights(self.outs, self.ins)\n",
        "        self._b = initialize_bias(self.outs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        helper method that computes the forward pass in the layer\n",
        "\n",
        "        Parameters:\n",
        "        x: a set of neuron states\n",
        "\n",
        "        Returns:\n",
        "        the next set of neuron states\n",
        "        \"\"\"\n",
        "        return self.act_function.f(np.dot(self._W, x) + self._b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NeuralNet:\n",
        "    \"\"\"\n",
        "    A series of layers connected and compatible.\n",
        "    \"\"\"\n",
        "    def __init__(self, layers, loss_function) -> None:\n",
        "        self._layers = layers\n",
        "        self._loss_function = loss_function\n",
        "        self.check_layer_compatibility()\n",
        "\n",
        "    def check_layer_compatibility(self):\n",
        "        for from_, to_ in zip(self._layers[:-1], self._layers[1:]):\n",
        "            print(\"from, to:\", from_.ins, to_.ins)\n",
        "            if from_.outs != to_.ins:\n",
        "                raise ValueError(\"Layers should have compatible shapes.\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x\n",
        "        for layer in self._layers:\n",
        "            out = layer.forward(out)\n",
        "        return out\n",
        "\n",
        "    def loss(self, y_pred, y):\n",
        "        return self._loss_function.loss(y_pred, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2-5. Generic NN demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "from, to: 2 4\n",
            "from, to: 4 4\n",
            "x:\n",
            " [[0.74922328]\n",
            " [0.9262836 ]]\n",
            "x.shape: (2, 1)\n",
            "\n",
            "y:\n",
            " [[-0.04547203]]\n",
            "loss:  0.0020677057794964925\n"
          ]
        }
      ],
      "source": [
        "def generic_nn_demo():\n",
        "    \"\"\"\n",
        "    Demo of a network as a serias of layers.\n",
        "    \"\"\"\n",
        "    input_shape = 2 # n_x\n",
        "    output_shape = 1 # n_y\n",
        "    net = NeuralNet(\n",
        "        [\n",
        "            Layer(input_shape, 4, LeakyReLU()),\n",
        "            Layer(4, 4, LeakyReLU()),\n",
        "            Layer(4, output_shape, LeakyReLU()),\n",
        "        ],\n",
        "        MSE()\n",
        "    )\n",
        "\n",
        "    # x\n",
        "    x = np.random.uniform(size=(input_shape, 1))\n",
        "    print(\"x:\\n\", x)\n",
        "    print(\"x.shape:\", x.shape)\n",
        "\n",
        "    # y \n",
        "    y = net.forward(x) \n",
        "    print(\"\\ny:\\n\", y)\n",
        "    print(\"loss: \", net.loss(y, np.array(0, ndmin=2)))\n",
        "\n",
        "generic_nn_demo()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Backward Propagation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NeuralNet:\n",
        "    \"\"\"\n",
        "    A series of layers connected and compatible.\n",
        "    \"\"\"\n",
        "    def __init__(self, layers, loss_function) -> None:\n",
        "        self._layers = layers\n",
        "        self._loss_function = loss_function\n",
        "        self.check_layer_compatibility()\n",
        "\n",
        "    def check_layer_compatibility(self):\n",
        "        for from_, to_ in zip(self._layers[:-1], self._layers[1:]):\n",
        "            print(\"from, to:\", from_.ins, to_.ins)\n",
        "            if from_.outs != to_.ins:\n",
        "                raise ValueError(\"Layers should have compatible shapes.\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        xs = [x]\n",
        "        for layer in self._layers:\n",
        "            xs.append(layer.forward(xs[-1]))\n",
        "        return xs\n",
        "\n",
        "    def loss(self, y_pred, y):\n",
        "        return self._loss_function.loss(y_pred, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3-1. The chain rule"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Unfold the recursive definition of $x_n$:\n",
        "$$L(x_n, t) = L(f_{n - 1}(W_{n - 1} \\cdot x_{n - 1} + b_{n - 1}), t)$$\n",
        "$$= L(f_{n - 1}(W_{n - 1} \\cdot f_{n - 2}(W_{n - 2} \\cdot x_{n - 2} + b_{n - 2}) + b_{n - 1}), t) \\\\ = \\dots $$\n",
        "\n",
        "Taking the partial derivative of $L$ with respect to $W_0$ will involve many more chain rules than with respect to $W_{n - 1}$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Unfold the recursive definition of $x_n$:\n",
        "\n",
        "$$L(a, \\space y) = L(f^{[l - 1]}(W^{[l - 1]} \\cdot a^{[l - 2]} + b^{[l - 1]}), \\space y)$$\n",
        "$$= L(f^{[l - 1]}(W^{[l - 1]} \\cdot f^{[l - 2]}(W^{[l - 2]} \\cdot a^{[l - 3]} + b^{[l - 2]}) + b^{[l - 1]}), \\space y) \\\\ \n",
        "= \\dots $$\n",
        "\n",
        "where $f^{[l]}$ is the activation function of $l^{th}$ layer\n",
        "\n",
        "Taking the partial derivative of $L$ with respect to $W^{[0]}$ will involve many more chain rules than with respect to $W^{[l - 1]}$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "from, to: 2 4\n",
            "from, to: 4 4\n"
          ]
        }
      ],
      "source": [
        "net = NeuralNet(\n",
        "    [\n",
        "        Layer(2, 4, LeakyReLU()),\n",
        "        Layer(4, 4, LeakyReLU()),\n",
        "        Layer(4, 1, LeakyReLU()),\n",
        "    ],\n",
        "    MSE()\n",
        ")\n",
        "n = len(net._layers)\n",
        "\n",
        "\n",
        "Ws = [layer._W for layer in net._layers]\n",
        "bs = [layer._b for layer in net._layers]\n",
        "fs = [layer.act_function.f for layer in net._layers]\n",
        "dfs = [layer.act_function.df for layer in net._layers]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "L = net._loss_function.loss\n",
        "dL = net._loss_function.dloss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3-1-1. First step\n",
        "$$\\frac {\\partial L} {\\partial x_{n - 1}}, \\frac {\\partial L} {\\partial W_{n - 1}}, \\frac {\\partial L} {\\partial b_{n - 1}}$$\n",
        "\n",
        "$$y_{n - 1} = W_{n - 1} \\cdot x_{n - 1} + b_{n - 1}$$\n",
        "$$\\frac {\\partial L} {\\partial b_{n - 1}} = dL(x_n, t) f^{'}_{n - 1}(y_{n - 1})$$\n",
        "$$\\frac {\\partial L} {\\partial x_{n - 1}} = W^{T}_{n - 1}\\frac {\\partial L} {\\partial b_{n - 1}}$$\n",
        "$$\\frac {\\partial L} {\\partial W_{n - 1}} = \\frac {\\partial L} {\\partial b_{n - 1}}x^{T}_{n - 1}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3-1-1. First step\n",
        "\n",
        "$$\\frac {\\partial L} {\\partial a^{[l - 1]}}, \\frac {\\partial L} {\\partial W^{[l - 1]}}, \\frac {\\partial L} {\\partial b^{[l - 1]}}$$\n",
        "\n",
        "$$z^{[l - 1]} = W^{[l - 1]} \\cdot a^{[l - 2]} + b^{[l - 1]}$$\n",
        "$$\\frac {\\partial L} {\\partial b^{[l - 1]}} = dL(\\hat y, y) da^{[l - 1]}$$\n",
        "$$\\frac {\\partial L} {\\partial a^{[l - 1]}} = W^{[l - 1]^T}\\frac {\\partial L} {\\partial b^{[l - 1]}}$$\n",
        "$$\\frac {\\partial L} {\\partial W^{[l - 1]}} = \\frac {\\partial L} {\\partial b^{[l - 1]}}x^{[l - 1]^T}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'xs' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m loss \u001b[38;5;241m=\u001b[39m L(fs[n \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m](np\u001b[38;5;241m.\u001b[39mdot(Ws[n \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m], \u001b[43mxs\u001b[49m[n \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m+\u001b[39m bs[n \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]), t)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'xs' is not defined"
          ]
        }
      ],
      "source": [
        "loss = L(fs[n - 1](np.dot(Ws[n - 1], xs[n - 1]) + bs[n - 1]), t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'xs' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(Ws[n \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m], xs[n \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m+\u001b[39m bs[n \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      3\u001b[0m dbs[n \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m dfs[n \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m](y) \u001b[38;5;241m*\u001b[39m dL(xs[n], t)\n\u001b[1;32m      4\u001b[0m dxs[n \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(Ws[n \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mT, dbs[n \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m])\n",
            "\u001b[0;31mNameError\u001b[0m: name 'xs' is not defined"
          ]
        }
      ],
      "source": [
        "y = np.dot(Ws[n - 1], xs[n - 1]) + bs[n - 1]\n",
        "\n",
        "dbs[n - 1] = dfs[n - 1](y) * dL(xs[n], t)\n",
        "dxs[n - 1] = np.dot(Ws[n - 1].T, dbs[n - 1])\n",
        "dWs[n - 1] = np.dot(dbs[n - 1], xs[n - 1].T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3-1-2. Next step\n",
        "#### Loss with dependence on xs[n - 2], Ws[n - 2] and bs[n - 2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\\frac {\\partial L} {\\partial x_{n - 2}}, \\frac {\\partial L} {\\partial W_{n - 2}}, \\frac {\\partial L} {\\partial b_{n - 2}}$$\n",
        "\n",
        "$$y_{n - 2} = W_{n - 2} \\cdot x_{n - 2} + b_{n - 2}$$\n",
        "$$\\frac {\\partial L} {\\partial b_{n - 2}} = \\frac {\\partial L} {\\partial x_{n - 1}} \\frac {\\partial x_{n - 1}} {\\partial b_{n - 2}} = \\frac {\\partial L} {\\partial x_{n - 1}} f^{'}_{n - 2}(y_{n - 2})$$\n",
        "$$\\frac {\\partial L} {\\partial x_{n - 2}} = \\frac {\\partial L} {\\partial x_{n - 1}} \\frac {\\partial x_{n - 1}} {\\partial x_{n - 2}} = \\frac {\\partial L} {\\partial x_{n - 1}} f^{'}_{n - 2}(y_{n - 2}) \\frac {\\partial y_{n - 2}} {\\partial x_{n - 2}}= W^{T}_{n - 2}\\frac {\\partial L} {\\partial b_{n - 2}}$$\n",
        "$$\\frac {\\partial L} {\\partial W_{n - 2}} = \\frac {\\partial L} {\\partial x_{n - 1}} \\frac {\\partial x_{n - 1}} {\\partial W_{n - 2}} = \\frac {\\partial L} {\\partial b_{n - 2}}x^{T}_{n - 2}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'xs' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[33], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m loss \u001b[38;5;241m=\u001b[39m L(fs[n \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m](\n\u001b[1;32m      2\u001b[0m     np\u001b[38;5;241m.\u001b[39mdot(\n\u001b[1;32m      3\u001b[0m         Ws[n \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m      4\u001b[0m         fs[n \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m](\n\u001b[1;32m      5\u001b[0m             np\u001b[38;5;241m.\u001b[39mdot(\n\u001b[1;32m      6\u001b[0m                 Ws[n \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m],\n\u001b[0;32m----> 7\u001b[0m                 xs[n \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m      8\u001b[0m             ) \u001b[38;5;241m+\u001b[39m bs[n \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m      9\u001b[0m         )\n\u001b[1;32m     10\u001b[0m     ) \u001b[38;5;241m+\u001b[39m bs[n \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     11\u001b[0m ), t)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'xs' is not defined"
          ]
        }
      ],
      "source": [
        "loss = L(fs[n - 1](\n",
        "    np.dot(\n",
        "        Ws[n - 1],\n",
        "        fs[n - 2](\n",
        "            np.dot(\n",
        "                Ws[n - 2],\n",
        "                xs[n - 2]\n",
        "            ) + bs[n - 2]\n",
        "        )\n",
        "    ) + bs[n - 1]\n",
        "), t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'xs' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(Ws[n \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m], xs[n \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m]) \u001b[38;5;241m+\u001b[39m bs[n \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m      3\u001b[0m dbs[n \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m=\u001b[39m dfs[n \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m](y) \u001b[38;5;241m*\u001b[39m dxs[n \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      4\u001b[0m dxs[n \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(Ws[n \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mT, dbs[n \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m])\n",
            "\u001b[0;31mNameError\u001b[0m: name 'xs' is not defined"
          ]
        }
      ],
      "source": [
        "y = np.dot(Ws[n - 2], xs[n - 2]) + bs[n - 2]\n",
        "\n",
        "dbs[n - 2] = dfs[n - 2](y) * dxs[n - 1]\n",
        "dxs[n - 2] = np.dot(Ws[n - 2].T, dbs[n - 2])\n",
        "dWs[n - 2] = np.dot(dbs[n - 2], xs[n - 2].T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### The General step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'xs' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(Ws[n \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m3\u001b[39m], xs[n \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m3\u001b[39m]) \u001b[38;5;241m+\u001b[39m bs[n \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m      3\u001b[0m dbs[n \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m=\u001b[39m dfs[n \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m3\u001b[39m](y) \u001b[38;5;241m*\u001b[39m dL(xs[n \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m], t)\n\u001b[1;32m      4\u001b[0m dxs[n \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(Ws[n \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39mT, dbs[n \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m3\u001b[39m])\n",
            "\u001b[0;31mNameError\u001b[0m: name 'xs' is not defined"
          ]
        }
      ],
      "source": [
        "y = np.dot(Ws[n - 3], xs[n - 3]) + bs[n - 3]\n",
        "\n",
        "dbs[n - 3] = dfs[n - 3](y) * dL(xs[n - 2], t)\n",
        "dxs[n - 3] = np.dot(Ws[n - 3].T, dbs[n - 3])\n",
        "dWs[n - 3] = np.dot(dbs[n - 3], xs[n - 3].T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'xs' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(Ws[\u001b[38;5;241m0\u001b[39m], xs[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m+\u001b[39m bs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      3\u001b[0m dbs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m dfs[\u001b[38;5;241m0\u001b[39m](y) \u001b[38;5;241m*\u001b[39m dL(xs[\u001b[38;5;241m1\u001b[39m], t)\n\u001b[1;32m      4\u001b[0m dxs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(Ws[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mT, dbs[\u001b[38;5;241m0\u001b[39m])\n",
            "\u001b[0;31mNameError\u001b[0m: name 'xs' is not defined"
          ]
        }
      ],
      "source": [
        "y = np.dot(Ws[0], xs[0]) + bs[0]\n",
        "\n",
        "dbs[0] = dfs[0](y) * dL(xs[1], t)\n",
        "dxs[0] = np.dot(Ws[0].T, dbs[0])\n",
        "dWs[0] = np.dot(dbs[0], xs[0].T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Generic loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'xs' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[37], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m dbs, dWs \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m----> 2\u001b[0m dxs \u001b[38;5;241m=\u001b[39m [dL(xs[n], t)]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, W, b, f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(xs[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], Ws, bs, fs))):\n\u001b[1;32m      5\u001b[0m     y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(W, x) \u001b[38;5;241m+\u001b[39m b\n",
            "\u001b[0;31mNameError\u001b[0m: name 'xs' is not defined"
          ]
        }
      ],
      "source": [
        "dbs, dWs = [], []\n",
        "dxs = [dL(xs[n], t)]\n",
        "\n",
        "for x, W, b, f in reversed(list(zip(xs[:-1], Ws, bs, fs))):\n",
        "    y = np.dot(W, x) + b\n",
        "    dbs.append(f(y) * dxs[-1])\n",
        "    dxs.append(np.dot(W.T, dbs[-1]))\n",
        "    dWs.append(np.dot(dbs[-1], x.T))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\\frac {\\partial L} {\\partial x_{n}} = dL(x_n, t)$$\n",
        "\n",
        "$$\\frac {\\partial L} {\\partial b_{n - i}} = \\frac {\\partial L} {\\partial x_{n - i + 1}} f^{'}_{n - i}(W_{n - i} \\cdot x_{n - i} + b_{n - i})$$\n",
        "$$\\frac {\\partial L} {\\partial x_{n - i}} = \\frac {\\partial L} {\\partial x_{n - i + 1}} \\frac {\\partial x_{n - i + 1}} {\\partial x_{n - i}} = \\frac {\\partial L} {\\partial x_{n - i + 1}} f^{'}_{n - i}(y_{n - i}) \\frac {\\partial y_{n - i}} {\\partial x_{n - i}}= W^{T}_{n - i}\\frac {\\partial L} {\\partial b_{n - i}}$$\n",
        "$$\\frac {\\partial L} {\\partial W_{n - i}} = \\frac {\\partial L} {\\partial x_{n - i + 1}} \\frac {\\partial x_{n - i + 1}} {\\partial W_{n - i}} = \\frac {\\partial L} {\\partial b_{n - i}}x^{T}_{n - i}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3-2. Training a neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NeuralNet:\n",
        "    \"\"\"\n",
        "    A series of layers connected and compatible.\n",
        "    \"\"\"\n",
        "    def __init__(self, layers, loss_function, lr) -> None:\n",
        "        self._layers = layers\n",
        "        self._loss_function = loss_function\n",
        "        self.lr = lr\n",
        "        self.check_layer_compatibility()\n",
        "\n",
        "    def check_layer_compatibility(self):\n",
        "        for from_, to_ in zip(self._layers[:-1], self._layers[1:]):\n",
        "            print(\"from, to:\", from_.ins, to_.ins)\n",
        "            if from_.outs != to_.ins:\n",
        "                raise ValueError(\"Layers should have compatible shapes.\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        # xs = [x]\n",
        "        # for layer in self._layers:\n",
        "        #     xs.append(layer.forward(xs[-1]))\n",
        "        # return xs\n",
        "        out = x\n",
        "        for layer in self._layers:\n",
        "            out = layer.forward(out)\n",
        "        return out\n",
        "\n",
        "    def loss(self, y_pred, y):\n",
        "        return self._loss_function.loss(y_pred, y)\n",
        "\n",
        "    def train(self, x, y):\n",
        "        \"\"\"\n",
        "        Train the network on input x and expected output y.\n",
        "        \"\"\"\n",
        "        # activations during forward pass\n",
        "        a = [x]\n",
        "        for layer in self._layers:\n",
        "            a.append(layer.forward(a[-1]))\n",
        "\n",
        "        # backpropagation\n",
        "        dz = a.pop() - y\n",
        "        m = y.shape[1]\n",
        "        n_layers = len(self._layers)\n",
        "        # print(\"m examples:\", m)\n",
        "        for i, (layer, a) in enumerate(zip(self._layers[::-1], a[::-1])):\n",
        "\n",
        "            # Compute the derivatives\n",
        "            # print(i)\n",
        "            dW = 1 / m * np.dot(dz, a.T)\n",
        "            db = 1 / m * np.sum(dz, axis=1, keepdims=True)\n",
        "            if i < n_layers: \n",
        "                dz = np.dot(layer._W.T, dz) * a * (1 - a)\n",
        "\n",
        "            # Update parameters\n",
        "            layer._W -= self.lr * dW\n",
        "            layer._b -= self.lr * db"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3-2. Generic Train demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "from, to: 2 4\n",
            "from, to: 4 4\n",
            "y:\n",
            " [[0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "loss:\n",
            " 0.6824644753009076\n",
            "loss:\n",
            " 2.8925836110343275e-07\n"
          ]
        }
      ],
      "source": [
        "def generic_train_demo():\n",
        "    \"\"\"\n",
        "    Demo of a network as a serias of layers.\n",
        "    \"\"\"\n",
        "    input_shape = 2\n",
        "    output_shape = 3\n",
        "\n",
        "    net = NeuralNet(\n",
        "        [\n",
        "            Layer(input_shape, 4, LeakyReLU()),\n",
        "            Layer(4, 4, LeakyReLU()),\n",
        "            Layer(4, output_shape, LeakyReLU()),\n",
        "        ],\n",
        "        MSE(), 0.001\n",
        "    )\n",
        "    y = np.zeros(shape=(output_shape, 1))\n",
        "    print(\"y:\\n\", y)\n",
        "\n",
        "\n",
        "    loss = 0\n",
        "    for _ in range(100):\n",
        "        x = np.random.normal(size=(input_shape, 1))\n",
        "        loss += net.loss(net.forward(x)[-1], y)\n",
        "    print(\"loss:\\n\", loss)\n",
        "\n",
        "    for _ in range(10000):\n",
        "        x = np.random.normal(size=(input_shape, 1))\n",
        "        net.train(x, y)\n",
        "\n",
        "    loss = 0\n",
        "    for _ in range(100):\n",
        "        x = np.random.normal(size=(input_shape, 1))\n",
        "        loss += net.loss(net.forward(x)[-1], y)\n",
        "    print(\"loss:\\n\", loss)\n",
        "\n",
        "generic_train_demo()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "42AI-cjung-mo",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
